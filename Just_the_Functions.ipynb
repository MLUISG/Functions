{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD6pNn+USJ2IYF3bF8sZqf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLUISG/Functions/blob/main/Just_the_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P5md8Xe0mSd"
      },
      "outputs": [],
      "source": [
        "def show_me(df):\n",
        "  \"\"\"Displays info, describe, head, and tail\n",
        "\n",
        "Money in the bank,\n",
        "Show me the stats and details,\n",
        "Info, head, tail, done.\"\"\"\n",
        "  print('Info')\n",
        "  print(df.info())\n",
        "  print('\\n\\n')\n",
        "  print('Described')\n",
        "  print(df.describe().T)\n",
        "  print('\\n\\n')\n",
        "  print('Head')\n",
        "  print(df.head().T)\n",
        "  print('\\n\\n')\n",
        "  print('Tail')\n",
        "  print(df.tail().T)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_df_by_dtype(df):\n",
        "    \"\"\"\n",
        "    Splits a Pandas DataFrame into two separate DataFrames:\n",
        "    one for columns with object dtype, and another for columns with non-object dtype.\n",
        "\n",
        "    Parameters:\n",
        "        - df (pandas.DataFrame): The DataFrame to split.\n",
        "\n",
        "    Returns:\n",
        "        - (pandas.DataFrame, pandas.DataFrame): A tuple containing two DataFrames:\n",
        "            1. The DataFrame containing the object columns.\n",
        "            2. The DataFrame containing the non-object columns.\n",
        "    \"\"\"\n",
        "    object_cols = df.select_dtypes(include='object')\n",
        "    non_object_cols = df.select_dtypes(exclude='object')\n",
        "    return object_cols, non_object_cols"
      ],
      "metadata": {
        "id": "mczbNPR-gXN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_tuple_cols(df):\n",
        "  \"\"\"Identifies tuples in all columns in a dataframe\n",
        "\n",
        "Tuple columns sought,\n",
        "Amidst the data's great breadth,\n",
        "Found and returned whole.\"\"\"\n",
        "  tuple_cols = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, tuple)).any()]\n",
        "  return tuple_cols"
      ],
      "metadata": {
        "id": "CmCKhbhVtThA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_tuples_to_lists(df):\n",
        "  \"\"\"Loops through columns, converting tuples\n",
        "\n",
        "From tuples to lists,\n",
        "DataFrame transformed with ease,\n",
        "Data now unbound.\"\"\"\n",
        "  for col in df.columns:\n",
        "      if type(df[col][0]) == tuple:\n",
        "          df[col] = df[col].apply(list)\n",
        "  return df"
      ],
      "metadata": {
        "id": "deKfmQO5tWwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(df):\n",
        "  \"\"\"Removes outliers from all columns in a dataframe with a threshold of 3* STD\n",
        "\n",
        "Outliers expelled,\n",
        "Data purged of impurities,\n",
        "Clean and pristine now.\"\"\"\n",
        "  cleaned_data = df\n",
        "  for col in df.columns:\n",
        "      mean = df[col].mean()\n",
        "      std = df[col].std()\n",
        "      threshold = 3 * std\n",
        "      lower_bound = mean - threshold\n",
        "      upper_bound = mean + threshold\n",
        "      cleaned_data[col] = df[col][(df[col] > lower_bound) & (df[col] < upper_bound)]\n",
        "  return cleaned_data"
      ],
      "metadata": {
        "id": "P35gOHNQtb4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_means(df):\n",
        "  \"\"\"Imputes the mean value of a column to all null values in that column\n",
        "\n",
        "Missing values found,\n",
        "Impute means to numeric cols,\n",
        "Clean data once more.\"\"\"\n",
        "  for col in df.columns:\n",
        "        if df[col].isna().sum() == 0:\n",
        "            continue\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            df[col].fillna(df[col].mean(), inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "ppmimZgdCsr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def all_the_value_counts(df):\n",
        "    \"\"\"Prints the value counts for all columns in a pandas DataFrame\n",
        "\n",
        "Counting values wide,\n",
        "All columns, one by one revealed,\n",
        "Insight to be found.\"\"\"\n",
        "    for column in df.columns:\n",
        "        print(f\"Value counts for {column}:\")\n",
        "        print(df[column].value_counts())\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "w_VVbOzx0vKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_the_cats(df):\n",
        "    \"\"\"Prints the value counts for categorical columns in a pandas DataFrame\n",
        "\n",
        "Meow, count the cats,\n",
        "Categorical values,\n",
        "Pandas purrs content.\n",
        "    \"\"\"\n",
        "    for column in df.select_dtypes(include=[\"category\", \"object\"]).columns:\n",
        "        print(f\"Value counts for {column}:\")\n",
        "        print(df[column].value_counts())\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "nVhq9bRxFmIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_nulls(data):\n",
        "    \"\"\"Checks for null values and displays only nulls\n",
        "\n",
        "Null values abound,\n",
        "Display them with a function,\n",
        "Data cleaning done.\"\"\"\n",
        "    temp = data.isna().sum()\n",
        "    filter = temp > 0\n",
        "    print(data.shape)\n",
        "    print(temp[filter])"
      ],
      "metadata": {
        "id": "oGQPX01X0w6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sorted_correlations(df):\n",
        "    corrs = df.corr().round(2).unstack().abs()\n",
        "    corrs = corrs[corrs < 1]\n",
        "    return corrs.sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "daRuycI8Ir_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def per_nulls(data):\n",
        "    \"\"\"Checks for null values and displays their percentage.\n",
        "\n",
        "Null values lurking,\n",
        "Percentage uncovered,\n",
        "Data now refined.\"\"\"\n",
        "    total = data.isna().sum().sort_values(ascending=False)\n",
        "    percent = round((data.isna().sum()/data.isna().count()*100), 2).sort_values(ascending=False)\n",
        "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    missing_data = missing_data[missing_data['Total'] > 0]\n",
        "    print(\"Missing data percentage:\\n\", missing_data)"
      ],
      "metadata": {
        "id": "DSdf5Jq3JmeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cull_nulls(df, threshold):\n",
        "    \"\"\"\n",
        "    Deletes columns from a Pandas DataFrame if they have more null values than the specified threshold.\n",
        "\n",
        "    Parameters:\n",
        "        - df (pandas.DataFrame): the DataFrame to process.\n",
        "        - threshold (float): the maximum percentage of null values allowed for a column to be kept.\n",
        "\n",
        "    Returns:\n",
        "        - pandas.DataFrame: the processed DataFrame.\n",
        "\n",
        "A data grave threat,\n",
        "Nulls swarm and infect the set,\n",
        "Cull them, make it clean.\n",
        "    \"\"\"\n",
        "    null_counts = df.isnull().sum()\n",
        "    null_percentages = null_counts / len(df) * 100\n",
        "    to_drop = null_percentages[null_percentages > threshold].index\n",
        "    return df.drop(to_drop, axis=1)"
      ],
      "metadata": {
        "id": "SqSbRe9HO80L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forest_features(df, target):\n",
        "  \"\"\"\n",
        "  This function uses random forest regression to predict a target variable from a given pandas DataFrame.\n",
        "It splits the data into training and testing sets, trains a random forest regressor model on the training set,\n",
        "and calculates the root mean squared error on the testing set. This process is repeated multiple times to\n",
        "calculate the mean RMSE. The function also plots the feature importance in descending order to\n",
        "provide insights into which features are most important for the model's predictions\n",
        "\n",
        "Random forest grows,\n",
        "Predicting with learned powers,\n",
        "Features, insights flow.\n",
        "  \"\"\"\n",
        "  X = df.drop(target, axis = 1)\n",
        "  y = df[target]\n",
        "  numLoops = 1\n",
        "\n",
        "  mean_error = np.zeros(numLoops)\n",
        "  np.random.seed(42)\n",
        "  for idx in range(0,numLoops):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
        "    model = RandomForestRegressor(n_estimators = 10, random_state=0)\n",
        "    model.fit(X_train,y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mean_error[idx] = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "  print(f'RMSE: {np.sqrt(mean_error).mean()}')\n",
        "  np.sqrt(mean_error)[:50]\n",
        "\n",
        "  importances = model.feature_importances_\n",
        "  importances\n",
        "  forest_importances = pd.Series(importances, index=X.columns)\n",
        "  forest_importances.sort_values(ascending=False, inplace=True)\n",
        "  plt.figure()\n",
        "  forest_importances.plot.bar()\n",
        "  plt.title(\"Feature importance\")\n",
        "  plt.ylabel(\"Mean decrease in impurity\")"
      ],
      "metadata": {
        "id": "eEu5oFVPPRaO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}